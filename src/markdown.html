<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>STATS NORMALITY ANALYSIS</title>

<link rel="stylesheet" type="text/css" href="extsyntax.css" />

</head>

<body>
<h1>STATS NORMALITY ANALYSIS</h1>

<p>A collection of statistics and plots for assessing univariate
and multivariate normality of a set of variables</p>
<div class="syntax">
<p>STATS NORMALITY ANALYSIS
VARIABLES variable list<sup>&#42;</sup><br/>
<p>/OUTPUT<br/>
UNIVARIATETESTS = AD SW CVM LILLIE SF<br/>
MVNTESTS = MARDIA HZ ROYSTON DH ENERGY</br>
BOOTSTRAPREPS = integer</br>
SCALEDATA = NO<sup>&#42;&#42;</sup> or YES<br/>
UNIPLOTS = YES<sup>&#42;&#42;</sup> or NO<BR/>
SCATTERPLOTS = NO<sup>&#42;&#42;</sup> or YES <BR/>
NSCATTERPLOTVARS number of variables for scattersA</br>
SCATTERPLOTSIZE number of pixels in each dimension for each scatterplot</p>
DESCRIPTIVES = NO<sup>&#42;&#42;</sup> or YES<BR/>
</p>
<p>
/OUTLIERS<BR/>
IDVAR = variable <sup>&#42;</sup><br/>
NOUTLIERS = integer <br/>
OUTLIERDETECTION = QUAN<sup>&#42;&#42;</sup> or ADJ
</p>

<p>/HELP</p>

<p><sup>&#42;</sup> Required<br/>
<sup>&#42;&#42;</sup> Default</p>
</div>
<p>STATS NORMALITY ANALYSIS /HELP displays this help and does nothing else.</p>


<pre class="example"><code>
STATS NORMALITY ANALYSIS VARIABLES = X1 TO X10
/ OUTPUT UNIVARIATE=AD SW.
</code></pre>


<H2>Subcommands and Keywords</H2>
<p>This procedure supports SPLIT FILES.  If scaling, each split is scaled separately. The procedure does not support case weights.  If weighting is on,
weights will be ignored with a warning.  Cases with any missing values in the selected variables
will be excluded from the analysis. SPSS date variables are treated as plain numbers  )the number of seconds since October 14, 1582)</p>

<p>
<strong>VARIABLES</strong> Specify the variables whose univariate and joint distributions are to
be analyzed.  At least two variables must be specified.
Only scale-level variables can be analyzed.  If SPLIT FILE is in effect, the
split variables cannot appear in the VARIABLES list.<p>
<p><strong>UNIVARIATETESTS</strong> Specify zero or more normality test statistics.
The available tests are as follows,
<ul>
<li>AD: Anderson-Darling</li>
<li>SW: Shapiro-Wilk.  Do not use this test if there are more than 5000 cases or fewer than 3.</li>
<li>CVM: Cramer-von Mises</li>
<li>SF: Shapiro-Francia</li>
<li>LILLIE: Lillefors (Kolmogorov-Smirnov)</li>
</ul>
<strong>MULTIVARIATETESTS</strong> Specify zero or more multivariate distribution tests.  
The available tests are as follows.
<ul>
<li>HZ: Henze-Zirkler</li>
<li>MARDIA: Mardia</li>
<li>ROYSTON: Royston</li>
<li>DH: Doornik-Hansen.  Do not use this test if there are more than 5000 cases or fewer than 3.</li>
<li>ENERGY: Energy E</li>
</ul>
<p><strong>BOOTSTRAPREPS</strong> specifies the number of replications for the bootstrap for the Energy E test.
Setting it to zero suppresses bootstrapping but means that no significance level will appear.</p>
<strong>UNIVARPLOTS</strong> specifies zero or more univariate plots.  
All the variables will appear in a single panel.  The plots are a box plot for all the variables
and pairs of a histogram and Q-Q plot for each variable.  If split files is on, a separate set of
plots for each split is produced.

<p>
<strong>SCATTERPLOTS</strong> 
produces a SPLOM of scatterplots for all the variables
<p><strong>OUTLIERS</strong></p>
A table and chart of multivariate outliers is specified in the <strong>OUTLIERS</strong> subcommand.
<P><strong>IDVAR</strong> specifies an ID variable that will be used to label the outliers and plot
points.  The id values must be unique across all the cases.</p>
<p><strong>OUTLIERDECTION</strong> specifies how outlier Mahalanobis distance is calculated.  The default
is QUAN.  With ADJ, an adaptive reweighted estimator for multivariate location and scatter with hard-rejection weights
is used.</p>
<strong>NOUTLIERS</strong> specifies the maximum number of outlier points to display.  In the table, the values are
shown in descending order of distance up to the limit.<p>

<h1>Information on the Tests and Plots</h1>
<p>A number of statistical tests, such as the Student's t-test and ANOVA assume
normality.  If this assumption is violated, the results may not be valid.  However,
in large samples, the Central Limit Theorem under general conditions 
states that the distribution of sample means approximately follows a normal
distribution, so the normality assumption is less important. </p>

<h2>Univariate Tests</h2>
<h3>Anderson-Darling Test</h3>
<p>
The Anderson Darling test is a statistical test that can be used to assess whether a data set follows a normal distribution or not. The test is based on the idea that if a data set is normally distributed, then the maximum difference between the cumulative distribution function of the data and the normal distribution should be minimized. So, the test statistic in this test is calculated based on the differences between the observed cumulative distribution function and the expected cumulative distribution function under the null hypothesis. The Anderson-Darling test is more sensitive to deviations from normality, especially in the tails of the distribution, making it better suited for detecting non-normal distributions. The p-value is used to determine the significance of the test statistic. A small p-value (typically less than 0.05 or 0.01) indicates that the null hypothesis can be rejected, suggesting that the sample does not follow the normal distribution.
</p>
<p><i>Example</i>:- The Anderson-Darling normality test finds extensive applications in quality control and process monitoring. In manufacturing and production environments, it is crucial to ensure that the processes are operating within specified limits and producing outputs that conform to quality standards. The Anderson-Darling test can be used to verify the normality assumption of the process data, which is a common requirement for many statistical process control techniques, such as control charts and so on. By checking the normality of process data, manufacturers can detect deviations from the expected distribution, which may indicate the presence of assignable causes or special causes of variation. This information can then be used to take corrective actions and bring the process back into a state of statistical control, ultimately improving product quality and reducing waste.</p>

<h3>Shapiro-Wilk Test</h3>
The Shapiro-Wilk test is used for testing univariate normality. This tests the null hypothesis that a set of samples came from a normally distributed population or not. The test statistic of Shapiro-Wilk test could be regarded as the ratio of two variance estimators, the best linear unbiased estimator (BLUE) and the maximum likelihood estimator (MLE). The p-value is used to determine the significance of the test statistic. A small p-value (typically less than 0.05 or 0.01) indicates that the null hypothesis can be rejected, suggesting that the sample does not follow the normal distribution.</p>

<p><i>Example</i>:- If a teacher wants to analyze the test scores of a set of students to determine if the scores are normally distributed, then the Shapiro-Wilk test can be used to assess this. And this assessment is important because many statistical analysis assume normality of the data.</p>

<h3>Shapiro-Francia Test</h3>
The Shapiro –Francia test is an approximate test that is similar to the Shapiro –Wilk test for very large samples. The test statistic is also different in both tests. Comparison studies have concluded that the Shapiro–Francia variant actually exhibits more power to distinguish some alternative hypothesis. The p-value is used to determine the significance of the test statistic. A small p-value (typically less than 0.05 or 0.01) indicates that the null hypothesis can be rejected, suggesting that the sample does not follow the normal distribution.</p>
<i>Example</i>:- If a company wants to analyze the customer satisfaction scores collected from various customers to determine if the scores are normally distributed, then Shapiro-Francia test could be used for this assessment of normality of data due to its computational efficiency for larger samples.</p>
<h3>Cramér-von Mises Test</h3>
<p>The Cramér-Von-Mises test evaluates the null hypothesis that a sample comes from a specified distribution, such as the normal distribution. It compares the empirical cumulative distribution function of the sample with the cumulative distribution function of the specified distribution. The test statistic is derived from the squared differences between the empirical cumulative distribution function (ECDF) of the sample and the cumulative distribution function (CDF) of the specified distribution, summed over all sample points. The p-value is used to determine the significance of the test statistic. A small p-value (typically less than 0.05 or 0.01) indicates that the null hypothesis can be rejected, suggesting that the sample does not follow the normal distribution.
</p>
<i>Example</i>:- If a retail company wants to analyze the annual sales figures of its stores to determine if the sales data is normally distributed, then Cramér-von Mises test can be applied for the assessment.</p>
<h3>Lilliefors (Kolmogorov-Smirnov) Test</h3>
<p>The Kolmogorov-Smirnov (K-S) test is one of the goodness-of-fit tests for normality. It quantifies the maximum distance between the empirical distribution function and the cumulative normal distribution. The Lilliefors test for normality is an improvement on the Kolmogorov-Smirnov test correcting the K-S for small values at the tails of probability distributions. The Kolmogorov-Smirnov test with Lilliefors significance correction is based on the greatest discrepancy between the sample cumulative distribution and the normal cumulative distribution. Lilliefors Kolmogorov-Smirnov test is used especially when the population mean and standard deviation are not known, but instead are estimated from the sample data. The p-value is used to determine the significance of the test statistic. A small p-value (typically less than 0.05 or 0.01) indicates that the null hypothesis can be rejected, suggesting that the sample does not follow the normal distribution.</p>

<p>This procedure only includes the Lillefors correction, since the distribution parameters are estimated.
If they are known, the SPSS Statistics Nonparametric Tests procedure can be used to
calculate the K-S test without the correction.</p>
<i>Example</i>:- If a company wants to analyze the productivity scores of employees to determine if the scores are normally distributed, then the Lilliefors (Kolmogorov-Smirnov) test can be used for this purpose.</p>

<h2>Univariate Plots</h2>
<h3>Histogram</h3>
Histogram is a very simple and important graph of the frequency distribution. The data is presented in the form of adjacent rectangles with height of rectangle proportional to the frequency. A normal curve is drawn over the histogram to examine if the data follows normal pattern.</p>
<h3>Box Plot</h3>
<p>
The box-whisker plot or box plot is another way to assess the normality of the data.. This plot uses the quartiles and extreme values of the data as a summary measure. The plot consists of a rectangle (the box) in the central part of the observed data and whiskers are drawn to the lowest and highest values from the rectangle. The limits of the box are lower and upper quartiles and the middle line is the median. A box plot that is symmetric with the median line at approximately the centre of the box and with symmetric whiskers indicate that the data may have come from a normal distribution. </p>
<h3>Q-Q Plot</h3>
The Quantile-Quantile (Q-Q) plot is also used for examining whether the data follows a specific distribution or not. The quantiles of the data are plotted against the expected values of desired distribution. This plot should look like a straight line. A quantile plot is a visual display that provides a lot of information about a univariate distribution and the quantiles of a distribution are a set of summary statistics that locate at relative positions within the complete ordered array of data values. For normally distributed data, observed data are approximate to the expected data, that is, they are statistically equal.</p>
<h3>Scatter Plot</h3>
A scatter plot is a graph that is used to observe and visually display the relationship between variables. The values of the variables are represented by dots. The positioning of the dots on the vertical and horizontal axis will inform the value of the respective data point and hence, scatter plots make use of Cartesian coordinates to display the values of the variables in a data set. 
</p>


<h2>Multivariate Normality Tests</h2>
<h3>Mardia Test</h3>
The Mardia test is a statistical test used to assess the multivariate normality of a dataset. This test assesses whether the skewness and kurtosis of the data match the skewness and kurtosis expected in a multivariate normal distribution. We have two test statistic in this test, Mardia test statistic for Skewness and Mardia test statistic for Kurtosis, where the skewness test statistic is approximately Chi square distributed and the Kurtosis test statistic is approximately normally distributed. The p-value is used to determine the significance of the test statistic. The test rejects the null hypothesis of normality if the p-value is less than 0.05 (or 0.01).</p>
<i>Example</i>:- If a financial analyst is examining the returns of a portfolio comprising stocks, bonds, and real estate assets over the past decade and wants to assess whether the multivariate distribution of the portfolio returns is approximately normal or not, then Mardia test could be used for this purpose. </p>
<h3>Royston Test</h3>
<p>The Royston test is the extension of Shapiro-Wilk test to the multivariate case. Royston transforms the m-Shapiro-Wilk statistics into an approximately Chi- squared random variable, with e (1 < e < m) degrees of freedom. The degrees of freedom are estimated by taking into account possible correlation structures between the original m test statistics. This test has been found to behave well when the sample size is small and the variates are relatively uncorrelated. The p-value is used to determine the significance of the test statistic. The test rejects the null hypothesis of normality if the p-value is less than 0.05 (or 0.01).</p>
<i>Example</i>:- In case of healthcare research, if a medical researcher is studying the relationship between several health indicators, such as blood pressure, cholesterol levels, and body mass index, in a sample of patients, then the researcher can apply the Royston test to check if the joint distribution of health indicators follows a multivariate normal distribution or not. </p>
<h3>Henze-Zirkler Test </h3>
<p>The Henze-Zirkler test computes a multivariate normality test statistic based on a non-negative functional distance that measures the distance between two distribution functions: the characteristic function of the multivariate normality and the empirical characteristic function. The Henze-Zirkler statistic is approximately distributed as a lognormal and the lognormal distribution is used to compute the null hypothesis probability. In other words, if the data is multivariate normal, the test statistic HZ is approximately lognormally distributed. It proceeds to calculate the mean, variance and the smoothness parameter. Then, the mean and variance are log normalized and the p-value is estimated. The p-value is used to determine the significance of the test statistic. The test rejects the null hypothesis of normality if the p-value is less than 0.05 (or 0.01).</p>
<i>Example</i>:- If a manufacturing company is interested in assessing the multivariate distribution of product dimensions to ensure quality control, then the quality control engineers can apply the Henze-Zirkler test to determine if the product dimensions follow a multivariate normal distribution. Deviations from normality may signal issues in the manufacturing process that need attention. </p>
<h3>Doornik-Hansen Test</h3>
<p>The Doornik-Hansen test for multivariate normality is based on the skewness and kurtosis of multivariate data that is transformed to ensure independence. The test statistic for this test is defined as the sum of squared transformations of the skewness and kurtosis, which approximately follows a Chi Square distribution. The p-value is used to determine the significance of the test statistic. The test rejects the null hypothesis of normality if the p-value is less than 0.05 (or 0.01).</p>
<i>Example</i>:- If a health researcher wants to assess the multivariate normality of three key health indicators such as Body Mass Index, Blood Pressure, and Cholesterol Level, then the Doornik-Hansen test can be applied to assess the multivariate normality of these set of health indicators.<p>
<h3>Energy Test</h3>
The energy test for multivariate normality provides a robust, non-parametric alternative that is particularly useful when dealing with high-dimensional datasets or when parametric assumptions are not met. The test statistic of the energy test for multivariate normality is based on the energy distance where the energy distance is simply a statistical measure that quantifies the distance between two probability distributions. For the multivariate normality test, the Euclidean distance is used to quantify the distance between observations within and between samples or distributions. Also, for this test, under the null hypothesis, the critical values are determined via simulation.</p> 
<i>Example</i>:- If an economist is analyzing a dataset consisting of various economic indicators, such as GDP growth rate, inflation rate, unemployment rate, and interest rate, from several countries and he wants to determine if these indicators follow a multivariate normal distribution, which would validate the use of certain multivariate statistical techniques, then the Energy test could be used to assess the multivariate normality of those economic indicators.</p>
<h3>Multivariate Plots</h3>
<p><strong>SCATTERPLOTS</strong> specifies whether or not to produce a SPLOM (scatterplot matrix) of the variables</p>
<p><strong>NSCATTERPLOTVARS</strong> can specify a limit, k, on the number of variables in the SPLOM.  The first k variables will be plotted.</p>
<p><strong>SCATTERPLOTSIZE</strong> specifies the size of each scatterplot in the SPLOM in pixels.  The default is 200.</p>
</h2>Outliers</h2>
<strong>IDVAR</strong> specifies an ID variable for use in the table of outliers.</P>
<strong>NOUTLIERS</strong> specifies the maximum number of outliers to be listed in the outliers table.  If zero, the table is not displayed.</p>
<p><strong>OUTLIERDETECTION</strong> specifies the method used to define the outlier distance.</p>


<h1>Acknowledgements</h1>
The calculations and plots are carried out by the R MVN package. The package authors are
Selcuk Korkmaz,
Dincer Goksuluk, and
Gokmen Zararsiz.
<p>&copy; Copyright(C) Jon K. Peck and IBM Corp, 2024</p>

</body>

</html>

